%\VignetteIndexEntry{flexsurv user guide}

%% TODO: run streg gen gamma on bc

\documentclass[nojss,nofooter]{jss}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{graphics}

\author{Christopher H. Jackson \\ MRC Biostatistics Unit, Cambridge, UK \\ \email{chris.jackson@mrc-bsu.cam.ac.uk}}
\title{flexsurv: a platform for parametric survival modelling in R}

\Abstract{ \pkg{flexsurv} is an R package for fully-parametric modelling of 
  survival data.  Any parametric time-to-event distribution
  may be fitted if the user supplies at minimum a probability density
  or hazard function.  Many standard survival distributions are built
  in, and also the three and four-parameter generalized gamma and F
  models.  Any parameter of the distribution can be modelled as a
  linear or log-linear function of covariates.  Another built-in model
  is the spline model of Royston and Parmar, in which both baseline
  survival and covariate effects can be arbitrarily flexible
  parametric functions of time.
 
  The main model-fitting function, \code{flexsurvreg}, uses the
  familiar syntax of \code{survreg} from the standard \pkg{survival}
  package --- censoring or left-truncation are specified in
  \code{Surv} objects.  Estimates and confidence intervals for any
  function of the model parameters can be printed or plotted.
  \pkg{flexsurv} also enhances the \pkg{mstate} package (Putter et al)
  by providing cumulative incidences for fully-parametric multi-state
  models.

  This article explains the methods and design principles of the
  package, giving several worked examples of its use.
}
\Keywords{survival}

\usepackage{Sweave}
\begin{document}

\section{Motivation and design}

The Cox model for survival data is ubiquitous in medical research, since the effects of
predictors can be estimated without needing to supply a
baseline survival distribution that might be inaccurate.  However,
fully-parametric models have many advantages, and even the originator
of the Cox model has expressed a preference for parametric modelling
\citep[see][]{reid:cox:conversation}.  Fully-specified models help to
understand the change in hazard through time, and help with prediction
and extrapolation. For example, the mean survival $E(T) =
\int_0^{\infty}S(t)$, used in health economic
evaluations \citep{latimer2013survival}, needs the survivor function
$S(t)$ to be fully-specified for all times $t$.

%% Cox "That's right, but since then various people have shown that
%% the answers are very insensitive to the parametric
%% formulation of the underlying distribution. And if you want
%% to do things like predict the outcome for a particular patient,
%% it's much more convenient to do that parametrically."

\pkg{flexsurv} allows parametric distributions of
arbitrary complexity to be fitted to survival data, gaining the
convenience of parametric modelling, while avoiding the risk of model
misspecification.  Built-in choices include splines with any number of
knots \citep{royston:parmar} and 3--4 parameter generalized gamma and
F distribution families.  Any user-defined model may be employed by
supplying at minimum an R function to compute the probability density
or hazard, and ideally also its cumulative form.  Any parameters may
be modelled in terms of covariates, and any function of the parameters
may be printed or plotted in model summaries.

\pkg{flexsurv} is intended as a general platform for survival
modelling in R.  The \code{survreg} function in the R package
\pkg{survival} \citep{therneau:survival} only supports two-parameter
(location/scale) distributions, though users can supply their own
distributions if they can be parameterised in this form.  Many other
contributed R packages can fit survival models, e.g. \pkg{eha}
\citep{eha}, \pkg{VGAM} \citep{yee:wild}, though these are either
limited to specific distribution families, not specifically designed
for survival analysis, or \citep[\pkg{ActuDistns}][]{actudistns}
contain only the definitions of distribution functions.
\pkg{flexsurv} enables distribution functions provided by such
packages to be used as survival models.

It is similar in spirit to the Stata packages \pkg{stpm2}
\citep{stpm2} for spline-based survival modelling, and \pkg{stgenreg}
\citep{stgenreg} for fitting survival models with user-defined hazard
functions using numerical integration.  Though in \pkg{flexsurv},
numerical integration can be avoided if the analytic cumulative
distribution or hazard can be supplied, and optimisation can also be
speeded by supplying analytic derivatives.  \pkg{flexsurv} also has
features for multi-state modelling and interval censoring, and general
output reporting.  It employs functional programming to work with
user-defined or existing R functions.



\section{General parametric survival model}

\subsection{Definitions} 

The general model that \pkg{flexsurv} fits has probability density function
\begin{equation}
  \label{eq:model}
  f(t | \mu(\mathbf{z}), \bm{\alpha}(\mathbf{z})), \quad t \geq 0  
\end{equation}

The cumulative distribution function $F(t)$, survivor
function $S(t) = 1 - F(t)$, cumulative hazard $H(t) = -\log S(t)$ and
hazard $h(t) = f(t)/S(t)$ are also defined (suppressing the conditioning for clarity).
$\mu=\alpha_0$ is the parameter of primary interest,
which usually governs the mean or location of the distribution.  Other
parameters $\bm{\alpha} = \alpha_1, \ldots, \alpha_R$ are called
``ancillary'' and determine the shape, variance or higher moments.

%%% Covariates may be time-dependent, but this notation generalizes to left-truncation, ref msm section 

\paragraph{Covariates} 

All parameters may depend on a vector of covariates $\mathbf{z}$
through link-transformed linear models $g_0(\mu) = \bm{\beta}_0^{'}
\mathbf{z}$ and $g_r(\alpha_r) = \bm{\beta}_r^{'} \mathbf{z}$. $g()$
will typically be $\log()$ if the parameter is defined to be positive,
or the identity function if the parameter is unrestricted.  In all
models, $\bm{\beta}$ includes at least an intercept, so that the full
set of parameters is given by $\{\bm{\beta}_r: r=0,\ldots,R$\}.

%% TODO use gamma for the intercept?

Suppose that the location, but not the ancillary parameters, depends
on covariates.  If the hazard function factorises as $h(t | \alpha,
\mu(\mathbf{z})) = \mu(\mathbf{z}) h_0(t | \alpha)$, then this is a
\emph{proportional hazards} (PH) model, so that the hazard ratio between
two groups (defined by different values of $\mathbf{z}$) is constant
over time.

Alternatively, if $S(t | \mu(\mathbf{z}), \alpha) =
S(\mu(\mathbf{z}) t | \alpha)$ then we have an \emph{accelerated
  failure time} (AFT) model, so that the effect of covariates is to speed or
slow the passage of time. For example, doubling the value of a
covariate with coefficient $\beta=\log(2)$ would give half the
expected survival time.


\paragraph{Data and likelihood} 

Let $t_i: i=1,\ldots, n$ be a sample of times from individuals $i$.
Let $c_i=1$ if $t_i$ is an observed death time, or $c_i=0$ if $t_i$ is
a right-censoring time, thus the true death time is known only to be
greater than $t_i$.  Also let $s_i$ be corresponding left-truncation
(or delayed-entry) times, meaning that individual $i$ is only observed
conditionally on having survived up to $s_i$, thus $s_i=0$ if there is
no left-truncation.  Additionally let $t^{max}_i$ be left-censoring
times.  If there is no left-censoring then these are infinite, so that
$S(t^{max}_i)=0$; or if the $i$th death time is interval-censored then
$c_i=0$ and $t^{max}_i$ is finite.

The likelihood for the parameters $\bm{\beta}$ in model
(\ref{eq:model}), given the corresponding data vectors, is
\begin{equation}
  \label{eq:lik}
  l(\{\bm{\beta}_r\} | \mathbf{t},\mathbf{c},\mathbf{s},\mathbf{t}^{max}) = \left\{ \prod_{i:\ c_i=1} f_i(t_i) \prod_{i:\ c_i=0} \left(S_i(t_i) - S_i(t^{max}_i)\right)\right\} / \prod_i S_i(s_i)  
\end{equation}

The individuals are independent, so that \pkg{flexsurv} does not
currently support frailty, clustered or random effects models.

An example dataset used throughout this paper is from 686 patients
with primary node positive breast cancer, available in the package as
\code{bc}. This was originally provided with \code{stpm} \citep{stpm:orig},
and analysed in much more detail by \citet{royston:parmar} and
\citet{sauerbrei1999building}.


\section{Model fitting syntax} 

The main model-fitting function is called \code{flexsurvreg}.  Its
first argument is an R \emph{formula} object.  The left hand side of
the formula gives the response as a survival object, using the
\code{Surv} function from the \pkg{survival} package.  Here, this
indicates that the response variable is \code{recyrs}, which represents
observed death or censoring times when the variable
\code{censrec} is 1 or 0 respectively.  The covariate \code{group} is
a factor representing a prognostic score, with three levels
\code{"Good"} (the baseline), \code{"Medium"} and
\code{"Poor"}. All of these variables are in the data frame
\code{bc}.
\begin{Schunk}
\begin{Sinput}
> library(flexsurv)
> fs1 <- flexsurvreg(Surv(recyrs, censrec) ~ group, data=bc, dist="weibull")
\end{Sinput}
\end{Schunk}

If we also had left-truncation times in a variable called
\code{start}, the response would be \\ \code{Surv(start,recyrs,censrec)}.
Or if all responses were interval-censored between lower and upper
bounds \code{tmin} and \code{tmax}, then we would write
\code{Surv(tmin,tmax,type="interval2")}.

If the argument \code{dist} is a string, this denotes a built-in
survival distribution.  In this case we fit a Weibull survival model.
Printing the fitted model object gives estimates and confidence
intervals for the model parameters and other useful information.  Note
that these are the \emph{same parameters} as represented by the R
distribution function \code{dweibull}: the \code{shape} $\alpha$ and
the \code{scale} $\mu$ of the survivor function $S(t) =
\exp(-(t/\mu)^\alpha)$, and \code{group} has a linear effect on
$\log(\mu)$.
\begin{Schunk}
\begin{Sinput}
> fs1
\end{Sinput}
\begin{Soutput}
Call:
flexsurvreg(formula = Surv(recyrs, censrec) ~ group, data = bc,     dist = "weibull")

Estimates: 
             data mean  est      L95%     U95%     se       exp(est)  L95%   
shape             NA     1.3797   1.2548   1.5170   0.0668       NA        NA
scale             NA    11.4229   9.1818  14.2110   1.2728       NA        NA
groupMedium   0.3338    -0.6136  -0.8623  -0.3649   0.1269   0.5414    0.4222
groupPoor     0.3324    -1.2122  -1.4583  -0.9661   0.1256   0.2975    0.2326
             U95%   
shape             NA
scale             NA
groupMedium   0.6943
groupPoor     0.3806

N = 686,  Events: 299,  Censored: 387
Total time at risk: 2113.425
Log-likelihood = -811.9419, df = 4
AIC = 1631.884
\end{Soutput}
\end{Schunk}
The same model can be fitted using \code{survreg} in 
\pkg{survival}:
\begin{Schunk}
\begin{Sinput}
> survreg(Surv(recyrs, censrec) ~ group, data=bc, dist="weibull")
\end{Sinput}
\begin{Soutput}
Call:
survreg(formula = Surv(recyrs, censrec) ~ group, data = bc, dist = "weibull")

Coefficients:
(Intercept) groupMedium   groupPoor 
  2.4356168  -0.6135892  -1.2122137 

Scale= 0.7248206 

Loglik(model)= -811.9   Loglik(intercept only)= -873.2
	Chisq= 122.53 on 2 degrees of freedom, p= 0 
n= 686 
\end{Soutput}
\end{Schunk}
The maximised log-likelihoods are the same, however the
parameterisation is different: the first coefficient
\code{(Intercept)} reported by \code{survreg} is $\log(\mu)$, and
\code{survreg}'s \code{"scale"} is \code{dweibull}'s (thus
\code{flexsurvreg})'s 1 / \code{shape}. The covariate effects
$\bm{\beta}$, however, have the same "accelerated failure time"
interpretation, as linear effects on $\log(\mu)$.  The multiplicative
effects $\exp(\bm{\beta})$ are printed in the output as
\code{exp(est)}.

\subsection{Built-in survival models}

\code{flexsurvreg}'s currently built-in distributions are listed in
Table \ref{tab:dists}.  In each case, the probability density $f()$
and parameters of the fitted model are taken from an existing R
function of the same name but beginning with the letter \code{d}.  For
the Weibull, exponential (\code{dexp}), gamma (\code{dgamma}) and
log-normal (\code{dlnorm}), the density functions are provided with
standard installations of R.  These density functions, and the
corresponding cumulative distribution function (with the first letter
\code{d} replaced by \code{p}) are used internally in
\code{flexsurvreg} to compute the likelihood.

\pkg{flexsurv} provides some additional survival distributions,
including a Gompertz distribution with unrestricted shape parameter
(\code{dist="gompertz"}), and the three- and four-parameter families
described below.  For all built-in distributions, \pkg{flexsurv} also
defines functions beginning \code{h} giving the hazard, and \code{H}
for cumulative hazard.

\paragraph{Generalized gamma} This three-parameter distribution
includes the Weibull, gamma and log-normal as special cases.  The
original parameterisation from \citet{stacy:gengamma} is available as\\
\code{dist="gengamma.orig"}, however the newer parameterisation
\citep{prentice:loggamma} is preferred: \code{dist="gengamma"}.  This has
parameters ($\mu$,$\sigma$,$q$), and survivor function
\[
\begin{array}{ll}
1 - I(\gamma,u)   & (q > 0)\\
1 - \Phi(z)  & (q = 0)\\
\end{array}
\]
where $I(a,x) = \int_0^x x^{a-1}\exp(-x)/\Gamma(a)$ is the incomplete gamma function (the cumulative gamma distribution with shape $a$ and scale 1), $\Phi$ is the standard normal cumulative distribution,  $u = \gamma \exp(|q|z)$, $z=(\log(t) - \mu)/\sigma$, and $\gamma=q^{-2}$.   The \citet{prentice:loggamma} parameterisation extends the original one to include a further class of models with negative $q$, and survivor function $I(\gamma,u)$, where $z$ is replaced by $-z$.   This stabilises estimation when the distribution is close to log-normal, since $q=0$ is no longer near the boundary of the parameter space.    In R notation, \footnote{The parameter called $q$ here and in previous literature is called $Q$ in \code{dgengamma} and related functions, since the first argument of a cumulative distribution function is conventionally named \code{q}, for quantile, in R.} the parameter values corresponding to the three special cases are

\begin{Code}
dgengamma(x, mu, sigma, Q=0)     ==  dlnorm(x, mu, sigma)                                
dgengamma(x, mu, sigma, Q=1)     ==  dweibull(x, shape=1/sigma, scale=exp(mu))           
dgengamma(x, mu, sigma, Q=sigma) ==  dgamma(x, shape=1/sigma^2, 
                                               rate=exp(-mu) / sigma^2)  
\end{Code}

The generalized gamma model is fitted to the breast cancer survival
data. \code{fs2} is an AFT model, where only the parameter
$\mu$ depends on the prognostic covariate \code{group}.  In a second
model \code{fs3}, the first ancillary parameter \code{sigma} ($\alpha_1$) also
depends on this covariate, giving a model with a time-dependent effect
that is neither PH nor AFT.  The second ancillary parameter \code{Q}
is still common between prognostic groups.
\begin{Schunk}
\begin{Sinput}
> fs2 <- flexsurvreg(Surv(recyrs, censrec) ~ group, data=bc, dist="gengamma")
> fs3 <- flexsurvreg(Surv(recyrs, censrec) ~ group + sigma(group), 
+                    data=bc, dist="gengamma")
\end{Sinput}
\end{Schunk}
Table~\ref{tab:aic} compares all the models fitted to the breast
cancer data, showing absolute fit to the data as measured by the
maximised -2$\times$ log likelihood $-2LL$, number of parameters $p$,
and Akaike's information criterion $-2LL + 2p$ which estimates
predictive ability.


\paragraph{Generalized F} This four-parameter distribution includes
the generalized gamma, and also the log-logistic, as special cases.
The variety of hazard shapes that can be represented is discussed by
\citet{ccox:genf}.  It is provided here in alternative ``original''
(\code{dist="genf.orig"}) and ``stable'' parameterisations
(\code{dist="genf"}) as presented by \citet{prentice:genf}. 
See \code{help(GenF)} and \code{help(GenF.orig)} in the package documentation 
for the exact definitions.


\begin{table}
  \begin{tabular}{llll}
\hline
    &  Parameters &  Density R function & \code{dist}\\
\hline
    Exponential & \code{rate}             & \code{dexp}   & \code{"exp"} \\
    Weibull     & \code{shape, scale}     & \code{dweibull} & \code{"weibull"} \\
    Gamma       & \code{shape, rate}      & \code{dgamma} & \code{"gamma"}\\
    Log-normal  & \code{meanlog, sdlog}   & \code{dlnorm} & \code{"lnorm"}\\
    Gompertz    & \code{shape, rate}      & \code{dgompertz} & \code{"gompertz"} \\
    Generalized gamma (Prentice 1975)   & \code{mu, sigma, Q} & \code{dgengamma} & \code{"gengamma"} \\
    Generalized gamma (Stacy 1962)& \code{shape, scale, k} & \code{dgengamma.orig} & \code{"gengamma.orig"} \\
    Generalized F     (stable)    & \code{mu, sigma, Q, P} & \code{dgenf} & \code{"genf"} \\
    Generalized F     (original)  & \code{mu, sigma, s1, s2} & \code{dgenf.orig} & \code{"genf.orig"} \\
\hline
  \end{tabular}
  \caption{Built-in parametric survival distributions in \pkg{flexsurv}}
  \label{tab:dists}
\end{table}

\subsection{Plotting outputs}

The \code{plot()} method for \code{flexsurvreg} objects is used as a
quick check of model fit.  By default, this draws a Kaplan-Meier
estimate of the survivor function $S(t)$, one for each combination of
categorical covariates, or just a single ``population average'' curve if there are no
categorical covariates.  The corresponding estimates from the fitted
model are overlaid.  Fitted values from further models can be added
with the \code{lines()} method.  
\begin{figure}[h]
  \centering
\begin{Schunk}
\begin{Sinput}
> plot(fs1, col="gray", lwd.obs=2)
> lines(fs2, col="red", lty=2)
> lines(fs3, col="red")
\end{Sinput}
\end{Schunk}
\includegraphics{flexsurv-005}
  \caption{Estimated survival from parametric models and Kaplan-Meier estimates.}
  \label{fig:surv}
\end{figure}

\code{scale="hazard"} can be used to plot hazards from parametric
models against kernel density estimates 
\citep[obtained from \pkg{muhaz},][]{muhaz,mueller:wang}.  This shows more clearly why the Weibull
model is inadequate: the hazard must be increasing or decreasing ---
while the generalized gamma can represent the increase and subsequent
decline in hazard seen in the data.
\begin{figure}[h]
  \centering
\begin{Schunk}
\begin{Sinput}
> plot(fs1, type="hazard", col="gray", lwd.obs=2)
> lines(fs2, type="hazard", col="red", lty=2)
> lines(fs3, type="hazard", col="red")
\end{Sinput}
\end{Schunk}
\includegraphics{flexsurv-006}
  \caption{Estimated hazards from parametric models and kernel density estimates.}
  \label{fig:surv}
\end{figure}

Similarly, \code{scale="cumhaz"} plots cumulative hazards. 
Confidence intervals are produced by simulating a large sample from
the asymptotic normal distribution of the maximum likelihood estimates
of $\{\bm{\beta}_r: r=0,\ldots,R$, via the function
\code{normboot.flexsurvreg}.

In this example, there is only a single categorical covariate, and the
\code{plot} and \code{summary} methods return one observed and fitted
trajectory for each level of that covariate.  For more complicated
models, users should specify exactly what covariate values they
want summaries for, rather than relying on the default \footnote{If there are only factor covariates, all combinations are plotted.  If
there are any continuous covariates, these methods by default return a ``population average''
curve, with the linear model design matrix set to its average
values, including the 0/1 contrasts defining factors, which doesn't
represent a meaningful covariate combination.}.
This is done by supplying the \code{newdata} argument, a 
data frame or list containing covariate values, just as
in standard R functions like \code{predict.lm}.

For more than casual plots, it is advised to set up the axes
beforehand, and use the \code{lines()} method.  Or for even more
flexibility, the data underlying the plots is available from the
\code{summary.flexsurvreg()} method.


\subsection{Custom model summaries}

Any function of the parameters of a fitted model can be summarised or plotted by
supplying the argument \code{fn} to \code{summary.flexsurvreg} or
\code{plot.flexsurvreg}.  This should be an R function, with mandatory
first two arguments \code{t} representing time, and \code{start}
representing a left-truncation point (so that the result is
conditional on survival up to that time). The remaining arguments must
be the parameters of the survival distribution.  For example, median 
survival under the Weibull model \code{fs1} can be summarised as follows
\begin{Schunk}
\begin{Sinput}
> median.weibull <- function(t, start, shape, scale) { 
+     qweibull(0.5, shape=shape, scale=scale) 
+ }
> summary(fs1, fn=median.weibull, t=1, B=10000)
\end{Sinput}
\begin{Soutput}
group=Good 
  time     est      lcl      ucl
1    1 8.75794 7.061168 10.77403

group=Medium 
  time      est      lcl      ucl
1    1 4.741585 4.126176 5.444223

group=Poor 
  time      est      lcl      ucl
1    1 2.605819 2.308508 2.938165
\end{Soutput}
\end{Schunk}
Although the median of the Weibull has an analytic form as $\mu
\log(2)^{1/\alpha}$, the form of the code given here generalises to
other distributions.
The argument \code{t} is not used in \code{median.weibull}, because
the median is a time-constant function of the parameters, unlike the
survival or hazard.  \code{10000} random samples are drawn to produce
a slightly more precise confidence interval than the default --- users
should adjust this until the desired level of precision is obtained.
A useful future extension of the package would be to allow users to 
supply derivatives of their custom summary function, so that the 
delta method can be used to obtain approximate confidence intervals 
without simulation.


\subsection{Computation}

The likelihood is maximised in \code{flexsurvreg} using the
optimisation methods available through the standard R \code{optim}
function.  By default, this is the \code{"BFGS"} method (\citep{nash})
using the analytic derivatives of the likelihood with respect to the
model parameters, if these are available, to improve the speed of
convergence to the maximum.  These are built-in for the exponential,
Weibull and Gompertz.  %% and spline models
For custom distributions, the user can optionally supply functions
with names beginning \code{"DLd"} and \code{"DLS"} respectively
(e.g. \code{DLdweibull,DLSweibull}) to calculate the derivatives of
the log density and log survivor functions with respect to the
transformed parameters $\gamma$.

Initial values are difficult: ideally two would come from moments of
the distribution, then defaults that reduce to simpler distributions.
example

\subsection{Custom survival distributions}
\label{sec:custom}

\pkg{flexsurv} is not limited to its built-in distributions.  Any
survival model of the form (\ref{eq:model}--\ref{eq:lik}) can be
fitted if we can provide either the density function $f()$ or the
hazard $h()$.  Many contributed R packages provide probability density
and cumulative distribution functions for positive distributions.  
Though survival models may be more naturally characterised by their
hazard function, representing the changing risk of death through time.
For example, for survival following major surgery we may want a
``U-shaped'' hazard curve, representing a high risk soon after the
operation, which then decreases, but increases naturally as survivors
grow older.

To supply a custom distribution, the \code{dist} argument to
\code{flexsurvreg} is defined to be an R list object, rather than a
character string.  The list has the following elements.

\begin{description}
\item[\code{name}] Name of the distribution.  For example, if this is \code{"llogis"} then there is assumed to be at least either 
  
  \begin{itemize}
  \item  a function called \code{dllogis} to compute the probability density, or 
  \item \code{hllogis} to compute the hazard.  
  \end{itemize}
  
  Ideally there will also be a function called \code{pllogis} for the
  cumulative distribution (if \code{d} is given), or \code{H} for the
  cumulative hazard (to complement \code{h}).
  
  These functions must be \emph{vectorised}, and the density function
  must also accept an argument \code{log}, which when \code{TRUE},
  returns the log density.  See the examples below.
  
\item[\code{pars}] Character vector naming the parameters of the
  distribution $\mu,\alpha_1,...,\alpha_R$.  These must match the
  arguments of the R distribution function or functions.
  
\item[\code{location}] Character: quoted name of the location parameter $\mu$.
  The location parameter will not necessarily be the first one, e.g. 
  in \code{dweibull} the \code{scale} comes after the \code{shape}.
  
\item[\code{transforms}] A list of functions $g()$ which transform the parameter from its natural range to the real line, for example, \code{c(log,identity)} \footnote{This is a \emph{list}, not an \emph{atomic vector} of functions, so if the distribution only has one parameter, we should write \code{transforms=c(log)} or \code{transforms=list(log)}, not \code{transforms=log}}

\item[\code{inv.transforms}] List of corresponding inverse functions.

\item[\code{inits}] A function which provides plausible initial values
  of the parameters for maximum likelihood estimation.  This is
  optional, but if not provided, then each call to \code{flexsurvreg}
  must have an \code{inits} argument containing a vector of initial
  values, which is inconvenient.  Implausible initial values may
  produce a likelihood of zero, and a fatal error message
  (\texttt{initial value in `vmmin' is not finite}) from the
  optimiser.
  
  Each distribution will ideally have a heuristic for initialising
  parameters from summaries of the data.  For example, since the
  median of the Weibull is $\mu \log(2)^{1/\alpha}$, a sensible
  estimate of $\mu$ will commonly be the median log uncensored
  survival time divided by $\log(2)$, with $\alpha=1$, assuming that
  in practice the true value of $\alpha$ is not often far from 1.  Then
  we define the function, of one argument \code{t} assumed to be the
  uncensored survival times, returning the initial values for the
  Weibull \code{shape} and \code{scale} respectively.

  \code{inits = function(t){ c(1, median(t[t>0]) / log(2)) } }
  
  More complicated initial value functions may use other data such
  as the covariate values and censored observations: for an example,
  see the function \code{flexsurv.splineinits} in the package source
  that computes initial values for spline models
  (\S\ref{sec:spline}).

\end{description}
    
\paragraph{Example: Using functions from a contributed package}

The following custom model uses the log-logistic distribution functions
(\code{dllogis} and \code{pllogis}) available in the package
\pkg{eha}.   The survivor function is $S(t|\mu,\alpha) = 1/(1 + (t/\mu)^\alpha)$,
so that the odds $(1-S(t))/S(t)$ of having died are a linear function of log time.
\begin{Schunk}
\begin{Sinput}
> library(eha)
> custom.llogis <- list(name="llogis",  pars=c("shape","scale"), location="scale",
+                       transforms=c(log, log), inv.transforms=c(exp, exp),
+                       inits=function(t){ c(1, median(t)) })
> fs4 <- flexsurvreg(Surv(recyrs, censrec) ~ group, data=bc, dist=custom.llogis)
\end{Sinput}
\end{Schunk}

This fits the breast cancer data better than the Weibull, since it can
represent a peaked hazard, but less well than the generalized gamma (Table \ref{tab:aic}).


\paragraph{Example: Wrapping functions from a contributed package}

Sometimes there may be probability density and similar functions in a
contributed package, but in a different format.  For example,
\pkg{eha} also provides a three-parameter Gompertz-Makeham
distribution with hazard $h(t|\mu,\alpha_1,\alpha_2)= \alpha_2 + \alpha_1 \exp(t/\mu)$.
The shape parameters $\alpha_1,\alpha_2$ are provided to 
\code{dmakeham} as a vector argument of length two.  However, \code{flexsurvreg}
expects distribution functions to have one argument for each
parameter.  Therefore we write our own functions that wrap around 
the third-party functions.
\begin{Schunk}
\begin{Sinput}
> dmakeham3 <- function(x, shape1, shape2, scale, ...)  {
+     dmakeham(x, shape=c(shape1, shape2), scale=scale, ...)
+ }
> pmakeham3 <- function(q, shape1, shape2, scale, ...)  {
+     pmakeham(q, shape=c(shape1, shape2), scale=scale, ...)
+ }
\end{Sinput}
\end{Schunk}
\code{flexsurvreg} also requires these functions to be
\emph{vectorized}, as the standard distribution functions in R are.
That is, we can supply a vector of alternative values for one or more
arguments, and expect a vector of the same length to be returned.  The
R base function \code{Vectorize} can be used to do this here.
\begin{Schunk}
\begin{Sinput}
> dmakeham3 <- Vectorize(dmakeham3) 
> pmakeham3 <- Vectorize(pmakeham3)
\end{Sinput}
\end{Schunk}
and this allows us to write, for example, 
\begin{Schunk}
\begin{Sinput}
> pmakeham3(c(0, 1, 1, Inf), 1, c(1, 1, 2, 1), 1)
\end{Sinput}
\begin{Soutput}
[1] 0.0000000 0.9340120 0.9757244 1.0000000
\end{Soutput}
\end{Schunk}
We could then use \code{dist=list(name="makeham3", pars=c("shape1","shape2","scale"),...)}
in a \code{flexsurvreg} model, though in the breast cancer example,
the second shape parameter is poorly identifiable.


\paragraph{Example: Changing the parameterisation of a distribution}

We may want to fit a Weibull model like \code{fs1}, but parameterised as $S(t) =
\exp(-\mu t^\alpha)$, so that the covariate effects reported in the
printed \code{flexsurvreg} object can be interpreted as hazard ratios
or log hazard ratios without any further transformation.
Here instead of the density and cumulative distribution functions, we
provide the hazard and cumulative hazard.\footnote{The \pkg{eha} package 
needs to be detached first so that \pkg{flexsurv}'s built-in \code{hweibull} is used, which returns \code{NaN} if the parameter values are zero, rather than failing as \pkg{eha}'s does.}
\begin{Schunk}
\begin{Sinput}
> detach("package:eha")
> hweibullPH <- function(x, shape, scale = 1, log=FALSE){
+     hweibull(x, shape=shape, scale=scale^{-1/shape}, log=log)
+ }
> HweibullPH <- function(x, shape, scale=1, log=FALSE){
+     Hweibull(x, shape=shape, scale=scale^{-1/shape}, log=log)
+ }
> custom.weibullPH <- list(name="weibullPH", 
+                          pars=c("shape","scale"), location="scale",
+                          transforms=c(log, log), inv.transforms=c(exp, exp),
+                          inits = function(t){
+                              c(1, median(t[t>0]) / log(2))
+                          })
> fs6 <- flexsurvreg(Surv(recyrs, censrec) ~ group, data=bc, dist=custom.weibullPH)
> 1 / fs1$res["scale","est"]^fs1$res["shape","est"]
\end{Sinput}
\begin{Soutput}
[1] 0.03472474
\end{Soutput}
\begin{Sinput}
> 1 / exp(fs1$res["groupMedium","est"]) ^ fs1$res["shape","est"]
\end{Sinput}
\begin{Soutput}
[1] 2.331564
\end{Soutput}
\end{Schunk}
The fitted model is the same as \code{fs1}, therefore the maximised likelihood is the same,
and the parameter estimates of \code{fs1} can be transformed to those of \code{fs6} as shown.

A slightly more complicated example is given in the examples vignette
of constructing a proportional hazards generalized gamma model.


\paragraph{Example: Omitting the cumulative distribution or hazard}

If there is no analytic form for $F(t)$ or $H(t)$ as the integral of
the density or hazard respectively, then \pkg{flexsurv} can compute
these internally by numerical integration, as in \pkg{stgenreg}
\citep{stgenreg}.  The default options of the built-in R routine
\code{integrate} for adaptive quadrature are used, though these may be
changed using the \code{integ.opts} argument to \code{flexsurvreg}.
Models specified this way will take much longer to fit, by an order of
magnitude.

EXAMPLE IN SECTION \ref{sec:gdim} 




\section{Any-dimension models}

\pkg{flexsurv} also supports models where the number of parameters is
arbitrary.  In the models discussed previously, the number of
parameters in the model family is fixed (e.g. three for the
generalized gamma).  In this section, the model complexity can be
chosen by the user, given the model family.  We may want to represent
more irregular hazard curves by more flexible functions, or use bigger
models if a bigger sample size makes it feasible to estimate more
parameters.


\subsection{Royston and Parmar spline model}
\label{sec:spline}

In the spline-based survival model of \citet{royston:parmar}, a
transformation $g(S(t,z))$ of the survival function is modelled as a
natural cubic spline function of log time, $x = \log(t)$, plus linear
effects of covariates $z$.  This is available here as the function
\code{flexsurvspline},  and is also available in the Stata package
\code{stpm2} \citep{stpm2} (and historically \code{stpm}, \citet{stpm:orig,stpm:update}).

  \[g(S(t,z)) = s(x, \bm{\gamma})\]

Typically we use $g(S(t,\mathbf{z}) = \log(-\log(S(t,\mathbf{z}))) =
\log(H(t,\mathbf{z}))$, the log cumulative hazard, giving a
proportional hazards model.    

\paragraph{Spline parameterisation}
The complexity of the model, thus the dimension of $\bm{\gamma}$, is
governed by the number of \emph{knots} $m$ in the spline function
$s()$.  Natural cubic splines are piecewise cubic polynomials defined
to be continuous, with continuous first and second derivatives at the
knots, and also constrained to be linear beyond boundary knots
$k_{min},k_{max}$.  As well as the boundary knots there may be up to
$m\geq 0$ \emph{internal} knots $k_1,\ldots k_m$.  Various spline
parameterisations exist --- the one used here is from
\citet{royston:parmar}.
\begin{equation}
  \label{eq:spline}
  s(x,\bm{\gamma}) = \gamma_0 + \gamma_1 x + \gamma_2 v_1(x) + \ldots + \gamma_{m+1} v_m(x)   
\end{equation}
where $v_j(x)$ is the $j$th \emph{basis} function

\[v_j(x) = (x - k_j)^3_+ - \lambda_j(x - k_{min})^3_+ - (1 - \lambda_j) (x - k_{max})^3_+, 
\qquad
\lambda_j = \frac{k_{max} - k_j}{k_{max} - k_{min}} \] 

and $(x - a)_+ = max(0, x - a)$.  If $m=0$ then there are only two
parameters $\gamma_0,\gamma_1$.  In fact if $g()$ is the log
cumulative hazard, this is equivalent to a Weibull model.  Table
\ref{tab:spline} explains two further choices of $g()$, and the
parameter values and distributions they simplify to for $m=0$.  
The probability density and cumulative distribution functions
for this model are available as \code{dsurvspline} and \code{psurvspline}

\begin{Scode}
  
\end{Scode}
  
  \begin{table}
  \begin{tabularx}{\textwidth}{lXll}
\hline
    Model &  $g(S(t,\mathbf{z}))$ & In \code{flexsurvspline} & With $m=0$ \\
\hline
    Proportional hazards & $\log(-\log(S(t,\mathbf{z})))$ \newline {\footnotesize (log cumulative hazard)}  & \code{scale="hazard"} & Weibull\\
%\multicolumn{4}{l}{{\footnotesize\code{pweibull(t, shape=a, scale=b) == psurvspline(t, gamma=c(log(1 / b^a), a))}}}\\
    Proportional odds    & $\log(S(t,\mathbf{z})^{-1} - 1)$ \newline {\footnotesize (log cumulative odds)}   & \code{scale="odds"} & Log-logistic\\
%\multicolumn{4}{l}{{\footnotesize\code{eha::pllogis(t, shape=a, scale=b) == psurvspline(t, gamma=c(-a*log(b), a), scale="odds")}}}\\
    Normal / probit      & $\Phi^{-1}(S(t,\mathbf{z}))$  \newline   {\footnotesize (inverse normal CDF, \code{qnorm})}    & \code{scale="normal"} & Log-normal \\  
%\multicolumn{4}{l}{{\footnotesize\code{plnorm(t, meanlog=a, sdlog=b) == psurvspline(t, gamma=c(-a/b, 1/b), scale="normal")}}}\\
\hline
  \end{tabularx}    
    \caption{Alternative modelling scales for \code{flexsurvspline}, and equivalent distribution families and parameter values for $m=0$ explained in R notation.}
    \label{tab:spline}
\end{table}

\paragraph{Covariates on spline parameters}
Covariates can be placed on any parameter $\gamma$ through a linear
model (with identity link function).  Most straightforwardly, we can
let the intercept $\gamma_0$ vary with covariates $\mathbf{z}$, giving
a proportional hazards or odds model (depending on $g()$).

\[g(S(t,z)) = s(x, \bm{\gamma}) + \bm{\beta}^T \mathbf{z} \]


The spline coefficients $\gamma_j: j=1, 2 \ldots$, the "ancillary parameters",
may also be modelled as linear functions of covariates $\mathbf{z}$, as

\[\gamma_j(\mathbf{z}) = \gamma_{j0} + \gamma_{j1}z_1 + \gamma_{j2}z_2 + \ldots\]

giving a model where the effects of covariates are arbitrarily flexible
functions of time: a non-proportional hazards or odds model.

\paragraph{Spline models in \pkg{flexsurv}}

The package provides the function \code{flexsurvspline} to fit this
general model. Internal knots are chosen by default from quantiles of
the log uncensored death times, however users can supply their own
knot locations in the \code{knots} argument to \code{flexsurvspline}.
Initial values for numerical likelihood maximisation are chosen using
the method described by \citet{royston:parmar} of Cox regression
combined with transforming an empirical survival estimate.

For example, the best-fitting model for the breast cancer dataset identified in \citet{royston:parmar},
a proportional odds model with one internal spline knot, is
\begin{Schunk}
\begin{Sinput}
> sp1 <- flexsurvspline(Surv(recyrs, censrec) ~ group, data=bc, k=1, 
+                       scale="odds")
\end{Sinput}
\end{Schunk}
A further model where the first ancillary parameter also depends on the prognostic
group, giving a time-varying odds ratio, is fitted as
\begin{Schunk}
\begin{Sinput}
> sp2 <- flexsurvspline(Surv(recyrs, censrec) ~ group + gamma1(group),
+                       data=bc, k=1, scale="odds")
\end{Sinput}
\end{Schunk}
These models give qualitatively similar results to the generalized
gamma in this dataset (Figure \ref{fig:spline:haz}), and have similar
predictive ability as measured by AIC (Table \ref{tab:aic}). Though in
general, an advantage of spline models is that extra flexibility is
available where necessary.

Note that the log hazard ratios under the proportional hazards spline
model are practically the same as under a standard Cox model.
\begin{Schunk}
\begin{Sinput}
> sp3 <- flexsurvspline(Surv(recyrs, censrec) ~ group, data=bc, k=1, scale="hazard")