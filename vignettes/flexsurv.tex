%\VignetteIndexEntry{flexsurv user guide}

\documentclass[nojss,nofooter]{jss}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{graphics}

\author{Christopher H. Jackson \\ MRC Biostatistics Unit, Cambridge, UK \\ \email{chris.jackson@mrc-bsu.cam.ac.uk}}
\title{flexsurv: a platform for parametric survival modelling in R}

\Abstract{ \pkg{flexsurv} is an R package for fully-parametric modelling of 
  survival data.  Any parametric time-to-event distribution
  may be fitted if the user supplies at minimum a probability density
  or hazard function.  Many standard survival distributions are built
  in, and also the three and four-parameter generalized gamma and F
  models.  Any parameter of the distribution can be modelled as a
  linear or log-linear function of covariates.  Another built-in model
  is the spline model of Royston and Parmar, in which both baseline
  survival and covariate effects can be arbitrarily flexible
  parametric functions of time.
 
  The main model-fitting function, \code{flexsurvreg}, uses the
  familiar syntax of \code{survreg} from the standard \pkg{survival}
  package --- censoring or left-truncation are specified in
  \code{Surv} objects.  Estimates and confidence intervals for any
  function of the model parameters can be printed or plotted.
  \pkg{flexsurv} also enhances the \pkg{mstate} package (Putter et al)
  by providing cumulative incidences for fully-parametric multi-state
  models.

  This article explains the methods and design principles of the
  package, giving several worked examples of its use.
}
\Keywords{survival}

\usepackage{Sweave}
\begin{document}

\section{Motivation and design}

The Cox model for survival data is ubiquitous in medical research, since the effects of
predictors can be estimated without needing to supply a
baseline survival distribution that might be inaccurate.  However,
fully-parametric models have many advantages, and even the originator
of the Cox model has expressed a preference for parametric modelling
\citep[see][]{reid:cox:conversation}.  Fully-specified models help to
understand the change in hazard through time, and help with prediction
and extrapolation. For example, the mean survival $E(T) =
\int_0^{\infty}S(t)$, used in health economic
evaluations \citep{latimer2013survival}, needs the survivor function
$S(t)$ to be fully-specified for all times $t$.

%% Cox "That's right, but since then various people have shown that
%% the answers are very insensitive to the parametric
%% formulation of the underlying distribution. And if you want
%% to do things like predict the outcome for a particular patient,
%% it's much more convenient to do that parametrically."

\pkg{flexsurv} allows parametric distributions of
arbitrary complexity to be fitted to survival data, gaining the
convenience of parametric modelling, while avoiding the risk of model
misspecification.  Built-in choices include splines with any number of
knots \citep{royston:parmar} and 3--4 parameter generalized gamma and
F distribution families.  Any user-defined model may be employed by
supplying at minimum an R function to compute the probability density
or hazard, and ideally also its cumulative form.  Any parameters may
be modelled in terms of covariates, and any function of the parameters
may be printed or plotted in model summaries.

\pkg{flexsurv} is intended as a general platform for survival
modelling in R.  The \code{survreg} function in the R package
\pkg{survival} \citep{therneau:survival} only supports two-parameter
(location/scale) distributions, though users can supply their own
distributions if they can be parameterised in this form.  Many other
contributed R packages can fit survival models, e.g. \pkg{eha}
\citep{eha}, \pkg{VGAM} \citep{yee:wild}, though these are either
limited to specific distribution families, not specifically designed
for survival analysis, or \citep[\pkg{ActuDistns}][]{actudistns}
contain only the definitions of distribution functions.
\pkg{flexsurv} enables distribution functions provided by such
packages to be used as survival models.

It is similar in spirit to the Stata packages \pkg{stpm2}
\citep{stpm2} for spline-based survival modelling, and \pkg{stgenreg}
\citep{stgenreg} for fitting survival models with user-defined hazard
functions using numerical integration.  Though in \pkg{flexsurv},
numerical integration can be avoided if the analytic cumulative
distribution or hazard can be supplied, and optimisation can also be
speeded by supplying analytic derivatives.  \pkg{flexsurv} also has
features for multi-state modelling and interval censoring, and general
output reporting.  It employs functional programming to work with
user-defined or existing R functions.



\section{General parametric survival model}

\subsection{Definitions} 

The general model that \pkg{flexsurv} fits has probability density function
\begin{equation}
  \label{eq:model}
  f(t | \mu(\mathbf{z}), \bm{\alpha}(\mathbf{z})), \quad t \geq 0  
\end{equation}

The cumulative distribution function $F(t)$, survivor
function $S(t) = 1 - F(t)$, cumulative hazard $H(t) = -\log S(t)$ and
hazard $h(t) = f(t)/S(t)$ are also defined (suppressing the conditioning for clarity).
$\mu=\alpha_0$ is the parameter of primary interest,
which usually governs the mean or location of the distribution.  Other
parameters $\bm{\alpha} = \alpha_1, \ldots, \alpha_R$ are called
``ancillary'' and determine the shape, variance or higher moments.

%%% Covariates may be time-dependent, but this notation generalizes to left-truncation, ref msm section 

\paragraph{Covariates} 

All parameters may depend on a vector of covariates $\mathbf{z}$
through link-transformed linear models $g_0(\mu) = \bm{\beta}_0^{'}
\mathbf{z}$ and $g_r(\alpha_r) = \bm{\beta}_r^{'} \mathbf{z}$. $g()$
will typically be $\log()$ if the parameter is defined to be positive,
or the identity function if the parameter is unrestricted.  In all
models, $\bm{\beta}$ includes at least an intercept, so that the full
set of parameters is given by $\{\bm{\beta}_r: r=0,\ldots,R$\}.

%% TODO use gamma for the intercept?

Suppose that the location, but not the ancillary parameters, depends
on covariates.  If the hazard function factorises as $h(t | \alpha,
\mu(\mathbf{z})) = \mu(\mathbf{z}) h_0(t | \alpha)$, then this is a
\emph{proportional hazards} (PH) model, so that the hazard ratio between
two groups (defined by different values of $\mathbf{z}$) is constant
over time.

Alternatively, if $S(t | \mu(\mathbf{z}), \alpha) =
S(\mu(\mathbf{z}) t | \alpha)$ then we have an \emph{accelerated
  failure time} (AFT) model, so that the effect of covariates is to speed or
slow the passage of time. For example, doubling the value of a
covariate with coefficient $\beta=\log(2)$ would give half the
expected survival time.


\paragraph{Data and likelihood} 

Let $t_i: i=1,\ldots, n$ be a sample of times from individuals $i$.
Let $c_i=1$ if $t_i$ is an observed death time, or $c_i=0$ if $t_i$ is
a right-censoring time, thus the true death time is known only to be
greater than $t_i$.  Also let $s_i$ be corresponding left-truncation
(or delayed-entry) times, meaning that individual $i$ is only observed
conditionally on having survived up to $s_i$, thus $s_i=0$ if there is
no left-truncation.  Additionally let $t^{max}_i$ be left-censoring
times.  If there is no left-censoring then these are infinite, so that
$S(t^{max}_i)=0$; or if the $i$th death time is interval-censored then
$c_i=0$ and $t^{max}_i$ is finite.

The likelihood for the parameters $\bm{\beta}$ in model
(\ref{eq:model}), given the corresponding data vectors, is
\begin{equation}
  \label{eq:lik}
  l(\{\bm{\beta}_r\} | \mathbf{t},\mathbf{c},\mathbf{s},\mathbf{t}^{max}) = \left\{ \prod_{i:\ c_i=1} f_i(t_i) \prod_{i:\ c_i=0} \left(S_i(t_i) - S_i(t^{max}_i)\right)\right\} / \prod_i S_i(s_i)  
\end{equation}

Note that the individuals are independent, so that \pkg{flexsurv} does not
currently support frailty, clustered or random effects models.

An example dataset used throughout this paper is from 686 patients
with primary node positive breast cancer, available in the package as
\code{bc}. This was originally provided with \code{stpm} \citep{stpm:orig},
and analysed in much more detail by \citet{royston:parmar} and
\citet{sauerbrei1999building}.


\section{Model fitting syntax} 

The main model-fitting function is called \code{flexsurvreg}.  Its
first argument is an R \emph{formula} object.  The left hand side of
the formula gives the response as a survival object, using the
\code{Surv} function from the \pkg{survival} package.  Here, this
indicates that the response variable is \code{recyrs}, which represents
observed death or censoring times when the variable
\code{censrec} is 1 or 0 respectively.  The covariate \code{group} is
a factor representing a prognostic score, with three levels
\code{"Good"} (the baseline), \code{"Medium"} and
\code{"Poor"}. All of these variables are in the data frame
\code{bc}.
\begin{Schunk}
\begin{Sinput}
> library(flexsurv)
> fs1 <- flexsurvreg(Surv(recyrs, censrec) ~ group, data=bc, dist="weibull")
\end{Sinput}
\end{Schunk}

If we also had left-truncation times in a variable called
\code{start}, the response would be \\ \code{Surv(start,recyrs,censrec)}.
Or if all responses were interval-censored between lower and upper
bounds \code{tmin} and \code{tmax}, then we would write
\code{Surv(tmin,tmax,type="interval2")}.

If the argument \code{dist} is a string, this denotes a built-in
survival distribution.  In this case we fit a Weibull survival model.
Printing the fitted model object gives estimates and confidence
intervals for the model parameters and other useful information.  Note
that these are the \emph{same parameters} as represented by the R
distribution function \code{dweibull}: the \code{shape} $\alpha$ and
the \code{scale} $\mu$ of the survivor function $S(t) =
\exp(-(t/\mu)^\alpha)$, and \code{group} has a linear effect on
$\log(\mu)$.
\begin{Schunk}
\begin{Sinput}
> fs1
\end{Sinput}
\begin{Soutput}
Call:
flexsurvreg(formula = Surv(recyrs, censrec) ~ group, data = bc,     dist = "weibull")

Estimates: 
             data mean  est      L95%     U95%     se       exp(est)  L95%   
shape             NA     1.3797   1.2548   1.5170   0.0668       NA        NA
scale             NA    11.4229   9.1818  14.2110   1.2728       NA        NA
groupMedium   0.3338    -0.6136  -0.8623  -0.3649   0.1269   0.5414    0.4222
groupPoor     0.3324    -1.2122  -1.4583  -0.9661   0.1256   0.2975    0.2326
             U95%   
shape             NA
scale             NA
groupMedium   0.6943
groupPoor     0.3806

N = 686,  Events: 299,  Censored: 387
Total time at risk: 2113.425
Log-likelihood = -811.9419, df = 4
AIC = 1631.884
\end{Soutput}
\end{Schunk}
The same model can be fitted using \code{survreg} in 
\pkg{survival}:
\begin{Schunk}
\begin{Sinput}
> survreg(Surv(recyrs, censrec) ~ group, data=bc, dist="weibull")
\end{Sinput}
\begin{Soutput}
Call:
survreg(formula = Surv(recyrs, censrec) ~ group, data = bc, dist = "weibull")

Coefficients:
(Intercept) groupMedium   groupPoor 
  2.4356168  -0.6135892  -1.2122137 

Scale= 0.7248206 

Loglik(model)= -811.9   Loglik(intercept only)= -873.2
	Chisq= 122.53 on 2 degrees of freedom, p= 0 
n= 686 
\end{Soutput}
\end{Schunk}
The maximised log-likelihoods are the same, however the
parameterisation is different: the first coefficient
\code{(Intercept)} reported by \code{survreg} is $\log(\mu)$, and
\code{survreg}'s \code{"scale"} is \code{dweibull}'s (thus
\code{flexsurvreg})'s 1 / \code{shape}. The covariate effects
$\bm{\beta}$, however, have the same "accelerated failure time"
interpretation, as linear effects on $\log(\mu)$.  The multiplicative
effects $\exp(\bm{\beta})$ are printed in the output as
\code{exp(est)}.

\subsection{Built-in survival models}

\code{flexsurvreg}'s currently built-in distributions are listed in
Table \ref{tab:dists}.  In each case, the probability density $f()$
and parameters of the fitted model are taken from an existing R
function of the same name but beginning with the letter \code{d}.  For
the Weibull, exponential (\code{dexp}), gamma (\code{dgamma}) and
log-normal (\code{dlnorm}), the density functions are provided with
standard installations of R.  These density functions, and the
corresponding cumulative distribution function (with the first letter
\code{d} replaced by \code{p}) are used internally in
\code{flexsurvreg} to compute the likelihood.

\pkg{flexsurv} provides some additional survival distributions,
including a Gompertz distribution with unrestricted shape parameter
(\code{dist="gompertz"}), and the three- and four-parameter families
described below.  For all built-in distributions, \pkg{flexsurv} also
defines functions beginning \code{h} giving the hazard, and \code{H}
for cumulative hazard.

\paragraph{Generalized gamma} This three-parameter distribution
includes the Weibull, gamma and log-normal as special cases.  The
original parameterisation from \citet{stacy:gengamma} is available as\\
\code{dist="gengamma.orig"}, however the newer parameterisation
\citep{prentice:loggamma} is preferred: \code{dist="gengamma"}.  This has
parameters ($\mu$,$\sigma$,$q$), and survivor function
\[
\begin{array}{ll}
1 - I(\gamma,u)   & (q > 0)\\
1 - \Phi(z)  & (q = 0)\\
\end{array}
\]
where $I(a,x) = \int_0^x x^{a-1}\exp(-x)/\Gamma(a)$ is the incomplete gamma function (the cumulative gamma distribution with shape $a$ and scale 1), $\Phi$ is the standard normal cumulative distribution,  $u = \gamma \exp(|q|z)$, $z=(\log(t) - \mu)/\sigma$, and $\gamma=q^{-2}$.   The \citet{prentice:loggamma} parameterisation extends the original one to include a further class of models with negative $q$, and survivor function $I(\gamma,u)$, where $z$ is replaced by $-z$.   This stabilises estimation when the distribution is close to log-normal, since $q=0$ is no longer near the boundary of the parameter space.    In R notation, \footnote{The parameter called $q$ here and in previous literature is called $Q$ in \code{dgengamma} and related functions, since the first argument of a cumulative distribution function is conventionally named \code{q}, for quantile, in R.} the parameter values corresponding to the three special cases are

\begin{Code}
dgengamma(x, mu, sigma, Q=0)     ==  dlnorm(x, mu, sigma)                                
dgengamma(x, mu, sigma, Q=1)     ==  dweibull(x, shape=1/sigma, scale=exp(mu))           
dgengamma(x, mu, sigma, Q=sigma) ==  dgamma(x, shape=1/sigma^2, 
                                               rate=exp(-mu) / sigma^2)  
\end{Code}

The generalized gamma model is fitted to the breast cancer survival
data. \code{fs2} is an AFT model, where only the parameter
$\mu$ depends on the prognostic covariate \code{group}.  In a second
model \code{fs3}, the first ancillary parameter \code{sigma} ($\alpha_1$) also
depends on this covariate, giving a model with a time-dependent effect
that is neither PH nor AFT.  The second ancillary parameter \code{Q}
is still common between prognostic groups.
\begin{Schunk}
\begin{Sinput}
> fs2 <- flexsurvreg(Surv(recyrs, censrec) ~ group, data=bc, dist="gengamma")
> fs3 <- flexsurvreg(Surv(recyrs, censrec) ~ group + sigma(group), 
+                    data=bc, dist="gengamma")
\end{Sinput}
\end{Schunk}
SHOW IT FITS BETTER, LIKS, AIC, PLOTS

\paragraph{Generalized F} This four-parameter distribution includes
the generalized gamma, and also the log-logistic, as special cases.
The variety of hazard shapes that can be represented is discussed by
\citet{ccox:genf}.  It is provided here in alternative ``original''
(\code{dist="genf.orig"}) and ``stable'' parameterisations
(\code{dist="genf"}) as presented by \citet{prentice:genf}. 
See \code{help(GenF)} and \code{help(GenF.orig)} in the package documentation 
for the exact definitions.


\begin{table}
  \begin{tabular}{llll}
\hline
    &  Parameters &  Density R function & \code{dist}\\
\hline
    Exponential & \code{rate}             & \code{dexp}   & \code{"exp"} \\
    Weibull     & \code{shape, scale}     & \code{dweibull} & \code{"weibull"} \\
    Gamma       & \code{shape, rate}      & \code{dgamma} & \code{"gamma"}\\
    Log-normal  & \code{meanlog, sdlog}   & \code{dlnorm} & \code{"lnorm"}\\
    Gompertz    & \code{shape, rate}      & \code{dgompertz} & \code{"gompertz"} \\
    Generalized gamma (Prentice 1975)   & \code{mu, sigma, Q} & \code{dgengamma} & \code{"gengamma"} \\
    Generalized gamma (Stacy 1962)& \code{shape, scale, k} & \code{dgengamma.orig} & \code{"gengamma.orig"} \\
    Generalized F     (stable)    & \code{mu, sigma, Q, P} & \code{dgenf} & \code{"genf"} \\
    Generalized F     (original)  & \code{mu, sigma, s1, s2} & \code{dgenf.orig} & \code{"genf.orig"} \\
\hline
  \end{tabular}
  \caption{Built-in parametric survival distributions in \pkg{flexsurv}}
  \label{tab:dists}
\end{table}

\subsection{Plotting outputs}

The \code{plot()} method for \code{flexsurvreg} objects is used as a
quick check of model fit.  By default, this draws a Kaplan-Meier
estimate of the survivor function $S(t)$, one for each combination of
categorical covariates, or just a single ``population average'' curve if there are no
categorical covariates.  The corresponding estimates from the fitted
model are overlaid.  Fitted values from further models can be added
with the \code{lines()} method.  
\begin{figure}[h]
  \centering
\begin{Schunk}
\begin{Sinput}
> plot(fs1, col="gray", lwd.obs=2)
> lines(fs2, col="red", lty=2)
> lines(fs3, col="red")
\end{Sinput}
\end{Schunk}
\includegraphics{flexsurv-005}
  \caption{Estimated survival from parametric models and Kaplan-Meier estimates.}
  \label{fig:surv}
\end{figure}

\code{scale="hazard"} can be used to plot hazards from parametric
models against kernel density estimates 
\citep[obtained from \pkg{muhaz},][]{muhaz,mueller:wang}.  This shows more clearly why the Weibull
model is inadequate: the hazard must be increasing or decreasing ---
while the generalized gamma can represent the increase and subsequent
decline in hazard seen in the data.
\begin{figure}[h]
  \centering
\begin{Schunk}
\begin{Sinput}
> plot(fs1, type="hazard", col="gray", lwd.obs=2)
> lines(fs2, type="hazard", col="red", lty=2)
> lines(fs3, type="hazard", col="red")
\end{Sinput}
\end{Schunk}
\includegraphics{flexsurv-006}
  \caption{Estimated hazards from parametric models and kernel density estimates.}
  \label{fig:surv}
\end{figure}

Similarly, \code{scale="cumhaz"} plots cumulative hazards. 
Confidence intervals are produced by simulating a large sample from
the asymptotic normal distribution of the maximum likelihood estimates
of $\{\bm{\beta}_r: r=0,\ldots,R$, via the function
\code{normboot.flexsurvreg}.

In this example, there is only a single categorical covariate, and the
\code{plot} and \code{summary} methods return one observed and fitted
trajectory for each level of that covariate.  For more complicated
models, users should specify exactly what covariate values they
want summaries for, rather than relying on the default \footnote{If there are only factor covariates, all combinations are plotted.  If
there are any continuous covariates, these methods by default return a ``population average''
curve, with the linear model design matrix set to its average
values, including the 0/1 contrasts defining factors, which doesn't
represent a meaningful covariate combination.}.
This is done by supplying the \code{newdata} argument, a 
data frame or list containing covariate values, just as
in standard R functions like \code{predict.lm}.

For more than casual plots, it is advised to set up the axes
beforehand, and use the \code{lines()} method.  Or for even more
flexibility, the data underlying the plots is available from the
\code{summary.flexsurvreg()} method.


\subsection{Custom model summaries}

Any function of the parameters of a fitted model can be summarised or plotted by
supplying the argument \code{fn} to \code{summary.flexsurvreg} or
\code{plot.flexsurvreg}.  This should be an R function, with mandatory
first two arguments \code{t} representing time, and \code{start}
representing a left-truncation point (so that the result is
conditional on survival up to that time). The remaining arguments must
be the parameters of the survival distribution.  For example, median 
survival under the Weibull model \code{fs1} can be summarised as follows
\begin{Schunk}
\begin{Sinput}
> median.weibull <- function(t, start, shape, scale) { 
+     qweibull(0.5, shape=shape, scale=scale) 
+ }
> summary(fs1, fn=median.weibull, t=1, B=10000)
\end{Sinput}
\begin{Soutput}
group=Good 
  time     est      lcl      ucl
1    1 8.75794 7.076969 10.78961

group=Medium 
  time      est      lcl      ucl
1    1 4.741585 4.121897 5.459531

group=Poor 
  time      est      lcl      ucl
1    1 2.605819 2.311678 2.929477
\end{Soutput}
\end{Schunk}
Although the median of the Weibull has an analytic form as $\mu
\log(2)^{1/\alpha}$, the form of the code given here generalises to
other distributions.
The argument \code{t} is not used in \code{median.weibull}, because
the median is a time-constant function of the parameters, unlike the
survival or hazard.  \code{10000} random samples are drawn to produce
a slightly more precise confidence interval than the default --- users
should adjust this until the desired level of precision is obtained.
A useful future extension of the package would be to allow users to 
supply derivatives of their custom summary function, so that the 
delta method can be used to obtain approximate confidence intervals 
without simulation.


\subsection{Computation}

The likelihood is maximised in \code{flexsurvreg} using the
optimisation methods available through the standard R \code{optim}
function.  By default, this is the \code{"BFGS"} method (\citep{nash})
using the analytic derivatives of the likelihood with respect to the
model parameters, if these are available, to improve the speed of
convergence to the maximum.  These are built-in for the exponential,
Weibull and Gompertz.  %% and spline models
For custom distributions, the user can optionally supply functions
with names beginning \code{"DLd"} and \code{"DLS"} respectively
(e.g. \code{DLdweibull,DLSweibull}) to calculate the derivatives of
the log density and log survivor functions with respect to the
transformed parameters $\gamma$.

Initial values are difficult: ideally two would come from moments of
the distribution, then defaults that reduce to simpler distributions.
example

\subsection{Custom survival distributions}

\pkg{flexsurv} is not limited to its built-in distributions.  Any
survival model of the form (\ref{eq:model}--\ref{eq:lik}) can be
fitted if we can provide either the density function $f()$ or the
hazard $h()$.  Many contributed R packages provide probability density
and cumulative distribution functions for positive distributions.  
Though survival models may be more naturally characterised by their
hazard function, representing the changing risk of death through time.
For example, for survival following major surgery we may want a
``U-shaped'' hazard curve, representing a high risk soon after the
operation, which then decreases, but increases naturally as survivors
grow older.

To supply a custom distribution, the \code{dist} argument to
\code{flexsurvreg} is defined to be an R list object, rather than a
character string.  The list has the following elements.

\begin{description}
\item[\code{name}] Name of the distribution.  For example, if this is \code{"llogis"} then there is assumed to be at least either 
  
  \begin{itemize}
  \item  a function called \code{dllogis} to compute the probability density, or 
  \item \code{hllogis} to compute the hazard.  
  \end{itemize}
  
  Ideally there will also be a function called \code{pllogis} for the
  cumulative distribution (if \code{d} is given), or \code{H} for the
  cumulative hazard (to complement \code{h}).
  
  These functions must be \emph{vectorised}, and the density function
  must also accept an argument \code{log}, which when \code{TRUE},
  returns the log density.  See the examples below.
  
\item[\code{pars}] Character vector naming the parameters of the
  distribution $\mu,\alpha_1,...,\alpha_R$.  These must match the
  arguments of the R distribution function or functions.
  
\item[\code{location}] Character: quoted name of the location parameter $\mu$.
  Note that the location parameter will not necessarily be the first one, e.g. 
  in \code{dweibull} the \code{scale} comes after the \code{shape}.
  
\item[\code{transforms}] A list of functions $g()$ which transform the parameter from its natural range to the real line, for example, \code{c(log,identity)} \footnote{Note this is a \emph{list}, not an \emph{atomic vector} of functions, so if the distribution only has one parameter, we should write \code{transforms=c(log)} or \code{transforms=list(log)}, not \code{transforms=log}}

\item[\code{inv.transforms}] List of corresponding inverse functions.

\item[\code{inits}] A function which provides plausible initial values
  of the parameters for maximum likelihood estimation.  This is
  optional, but if not provided, then each call to \code{flexsurvreg}
  must have an \code{inits} argument containing a vector of initial
  values, which is inconvenient.
  
  Each distribution will ideally have a heuristic for initialising
  parameters from summaries of the data.  For example, the median of
  the Weibull is $\mu \log(2)^{1/\alpha}$, a sensible estimate of
  $\mu$ will commonly be the median log uncensored survival time
  divided by $\log(2)$, with $\alpha=1$, assuming that in practice the
  true value of $\alpha$ is not too far from 1.  Then we define the
  function, of one argument \code{t} assumed to be the uncensored
  survival times, returning the initial values for the Weibull
  \code{shape} and \code{scale} respectively.

\code{inits = function(t){ c(1, median(t[t>0]) / log(2))) }
  
  More complicated initial value functions may use other data such
  as the covariate values and censored observations: for an example,
  see the function \code{flexsurv.splineinits} in the package source
  that computes initial values for spline models
  (\S\ref{sec:spline}).

\end{description}
    
\paragraph{Example: Using functions from a contributed package}

The following custom model uses the log-logistic distribution functions
(\code{dllogis} and \code{pllogis}) available in the package
\pkg{eha}.   The survivor function is $S(t|\mu,\alpha) = 1/(1 + (t/\mu)^\alpha)$,
so that the odds $(1-S(t))/S(t)$ of having died are a linear function of log time.
\begin{Schunk}
\begin{Sinput}
> library(eha)
> custom.llogis <- list(name="llogis",  pars=c("shape","scale"), location="scale",
+                       transforms=c(log, log), inv.transforms=c(exp, exp),
+                       inits=function(t){ c(1, median(t)) })
> fs4 <- flexsurvreg(Surv(recyrs, censrec) ~ group, data=bc, dist=custom.llogis)
\end{Sinput}
\end{Schunk}

This fits the breast cancer data better than the Weibull, since it can
represent a peaked hazard, but less well than the generalized gamma.
Proportional odds.  ANY SPACE FOR A HIGHLIGHTED PLOT



\paragraph{Example: Wrapping functions from a contributed package}

Sometimes there may be probability density and similar functions in a
contributed package, but in a different format.  For example,
\pkg{eha} also provides a three-parameter Gompertz-Makeham
distribution with density \code{dmakeham}, and 
hazard $h(t|\mu,\alpha_1,\alpha_2)= \alpha_2 + \alpha_1 \exp(t/\mu)$
whose shape parameters ($(\alpha_1,\alpha_2)$) are
provided as a vector of length two.  However, \code{flexsurvreg}
expects its distribution functions to have one argument for each
parameter.  Therefore we write our own functions that wrap around 
the third-party functions.
\begin{Schunk}
\begin{Sinput}
> dmakeham3 <- function(x, shape1, shape2, scale, ...)  {
+     dmakeham(x, shape=c(shape1, shape2), scale=scale, ...)
+ }
> pmakeham3 <- function(q, shape1, shape2, scale, ...)  {
+     pmakeham(q, shape=c(shape1, shape2), scale=scale, ...)
+ }
\end{Sinput}
\end{Schunk}
\code{flexsurvreg} also requires these functions to be
\emph{vectorized}, as the standard distribution functions in R are.
That is, we can supply a vector of alternative values for one or more
arguments, and expect a vector of the same length to be returned.  The
R base function \code{Vectorize} can be used to do this here.
\begin{Schunk}
\begin{Sinput}
> dmakeham3 <- Vectorize(dmakeham3) 
> pmakeham3 <- Vectorize(pmakeham3)
\end{Sinput}
\end{Schunk}
and this allows us to write, for example, 
\begin{Schunk}
\begin{Sinput}
> pmakeham3(c(0, 1, 1, Inf), 1, c(1, 1, 2, 1), 1)
\end{Sinput}
\begin{Soutput}
[1] 0.0000000 0.9340120 0.9757244 1.0000000
\end{Soutput}
\end{Schunk}
We could then use \code{dist=list(name="makeham3", pars=c("shape1","shape2","scale"),...)}
in a \code{flexsurvreg} model\footnote{though the second shape parameter appears to be not 
  identifiable in the breast cancer example}. 


\paragraph{Example: Changing the parameterisation of a distribution}

We may want to fit a Weibull model like \code{fs1}, but parameterised as $S(t) =
\exp(-\mu t^\alpha)$, so that the covariate effects reported in the
printed \code{flexsurvreg} object can be interpreted as hazard ratios
or log hazard ratios without any further transformation.
Here instead of the density and cumulative distribution functions, we
provide the hazard and cumulative hazard.\footnote{The \pkg{eha} package 
needs to be detached first so that \pkg{flexsurv}'s built-in \code{hweibull} is used, which returns \code{NaN} if the parameter values are zero, rather than failing as \pkg{eha}'s does}
\begin{Schunk}
\begin{Sinput}
> detach("package:eha")
> hweibullPH <- function(x, shape, scale = 1, log=FALSE){
+     hweibull(x, shape=shape, scale=scale^{-1/shape}, log=log)
+ }
> HweibullPH <- function(x, shape, scale=1, log=FALSE){
+     Hweibull(x, shape=shape, scale=scale^{-1/shape}, log=log)
+ }
> custom.weibullPH <- list(name="weibullPH", 
+                          pars=c("shape","scale"), location="scale",
+                          transforms=c(log, log), inv.transforms=c(exp, exp),
+                          inits = function(t){
+                              c(1, median(t[t>0]) / log(2))
+                          })
> fs6 <- flexsurvreg(Surv(recyrs, censrec) ~ group, data=bc, dist=custom.weibullPH)
> 1 / fs1$res["scale","est"]^fs1$res["shape","est"]
\end{Sinput}
\begin{Soutput}
[1] 0.03472474
\end{Soutput}
\begin{Sinput}
> 1 / exp(fs1$res["groupMedium","est"]) ^ fs1$res["shape","est"]
\end{Sinput}
\begin{Soutput}
[1] 2.331564
\end{Soutput}
\end{Schunk}
The fitted model is the same as \code{fs1}, therefore the maximised likelihood is the same,
and the parameter estimates of \code{fs1} can be transformed to those of \code{fs6} as shown.

A slightly more complicated example is given in the examples vignette
of constructing a proportional hazards generalized gamma model.


\paragraph{Example: Omitting the cumulative distribution or hazard}

If there is no analytic form for $F(t)$ or $H(t)$ as the integral of
the density or hazard respectively, then \pkg{flexsurv} can compute
these internally by numerical integration, though this will
substantially slow down the computation.  The default options of the
built-in R routine \code{integrate} for adaptive quadrature are used,
though these may be changed using the \code{integ.opts} argument to
\code{flexsurvreg}.

EXAMPLE IN SECTION \ref{sec:gdim} 




\section{Any-dimension models}

\pkg{flexsurv} also supports models where the number of parameters is
arbitrary.  In the models discussed previously, the number of
parameters in the model family is fixed (e.g. three for the
generalized gamma).  In this section, the model complexity can be
chosen by the user, given the model family.  We may want to represent
more irregular hazard curves by more flexible functions, or use bigger
models if a bigger sample size makes it feasible to estimate more
parameters.


\subsection{Royston and Parmar spline model}
\label{sec:spline}

In the spline-based survival model of \citet{royston:parmar}, a
transformation $g(S(t,z))$ of the survival function is modelled as a
natural cubic spline function of log time, $x = \log(t)$, plus linear
effects of covariates $z$.  This is available here as the function
\code{flexsurvspline},  and is also available in the Stata package
\code{stpm2} \citep{stpm2} (and historically \code{stpm}, \citet{stpm:orig,stpm:update}).

  \[g(S(t,z)) = s(x, \bm{\gamma})\]

Typically we use $g(S(t,\mathbf{z}) = \log(-\log(S(t,\mathbf{z}))) =
\log(H(t,\mathbf{z}))$, the log cumulative hazard, giving a
proportional hazards model.    

\paragraph{Spline parameterisation}
The complexity of the model, thus the dimension of $\bm{\gamma}$, is
governed by the number of \emph{knots} $m$ in the spline function
$s()$.  Natural cubic splines are piecewise cubic polynomials defined
to be continuous, with continuous first and second derivatives at the
knots, and also constrained to be linear beyond boundary knots
$k_{min},k_{max}$.  As well as the boundary knots there may be up to
$m\geq 0$ \emph{internal} knots $k_1,\ldots k_m$.  Various spline
parameterisations exist --- the one used here is from
\citet{royston:parmar}.
\begin{equation}
  \label{eq:spline}
  s(x,\bm{\gamma}) = \gamma_0 + \gamma_1 x + \gamma_2 v_1(x) + \ldots + \gamma_{m+1} v_m(x)   
\end{equation}
where $v_j(x)$ is the $j$th \emph{basis} function

\[v_j(x) = (x - k_j)^3_+ - \lambda_j(x - k_{min})^3_+ - (1 - \lambda_j) (x - k_{max})^3_+, 
\qquad
\lambda_j = \frac{k_{max} - k_j}{k_{max} - k_{min}} \] 

and $(x - a)_+ = max(0, x - a)$.  If $m=0$ then there are only two
parameters $\gamma_0,\gamma_1$ --- in fact if $g()$ is the log
cumulative hazard, this is equivalent to a Weibull model (PARAMETERS /
DPQR STATEMENT).  Table \ref{tab:spline} explains two alternative
choices of $g()$.
  
  \begin{table}
  \begin{tabularx}{\textwidth}{lXll}
\hline
    Model &  $g(S(t,\mathbf{z}))$ & In \code{flexsurvspline} & With $m=0$ \\
\hline
    Proportional hazards & $\log(-\log(S(t,\mathbf{z})))$ \newline {\footnotesize (log cumulative hazard)}  & \code{scale="hazard"} & Weibull\\
    Proportional odds    & $\log(S(t,\mathbf{z})^{-1} - 1)$ \newline {\footnotesize (log cumulative odds)}   & \code{scale="odds"} & Log-logistic\\
    Normal / probit      & $\Phi^{-1}(S(t,\mathbf{z}))$  \newline   {\footnotesize (inverse normal CDF, \code{qnorm})}    & \code{scale="normal"} & Log-normal \\  
\hline
  \end{tabularx}    
    \caption{Alternative modelling scales for \code{flexsurvspline}}
    \label{tab:spline}
\end{table}

\paragraph{Covariates on spline parameters}
Covariates can be placed on any parameter $\gamma$ through a linear
model (with identity link function).  Most straightforwardly we can
let the intercept $\gamma_0$ vary with covariates $\mathbf{z}$, giving
a proportional hazards or odds model (depending on $g()$).

\[g(S(t,z)) = s(x, \bm{\gamma}) + \bm{\beta}^T \mathbf{z} \]


The spline coefficients $\gamma_j: j=1, 2 \ldots$, the "ancillary parameters",
may also be modelled as linear functions of covariates $\mathbf{z}$, as

\[\gamma_j(\mathbf{z}) = \gamma_{j0} + \gamma_{j1}z_1 + \gamma_{j2}z_2 + \ldots\]

giving a model where the effects of covariates are arbitrarily flexible
functions of time: a non-proportional hazards or odds model.

\paragraph{Spline models in \pkg{flexsurv}}

The package provides the function \code{flexsurvspline} to fit this general 
model. 
For example, the best-fitting model investigated by \citet{royston:parmar},
a proportional odds model with one internal spline knot, can be fitted 
as follows.
\begin{Schunk}
\begin{Sinput}
> sp1 <- flexsurvspline(Surv(recyrs, censrec) ~ group, data=bc, k=1, 
+                       scale="odds")
\end{Sinput}
\end{Schunk}
A further model where the first ancillary parameter also depends on the prognostic
group, giving a time-varying odds ratio, is fitted as
\begin{Schunk}
\begin{Sinput}
> sp2 <- flexsurvspline(Surv(recyrs, censrec) ~ group + gamma1(group),
+                       data=bc, k=1, scale="odds")
\end{Sinput}
\end{Schunk}
These models give qualitatively similar results to the generalized gamma
in this dataset (Figure \ref{fig:spline:haz}).   AIC REF AIC TABLE TODO
\begin{figure}[h]
  \centering
\begin{Schunk}
\begin{Sinput}
> plot(sp1, type="hazard", ylim=c(0, 0.5), xlab="Years since")
> lines(sp2, type="hazard", col="red", lty.fit=2)
> lines(fs2, type="hazard", col="blue")
\end{Sinput}
\end{Schunk}
\includegraphics{flexsurv-015}
  \caption{Comparison of spline and generalized gamma fitted hazards}
  \label{fig:spline:haz}
\end{figure}

Though in general, an advantage of spline models is that extra
flexibility is available where necessary.  The internal knots are
chosen by default from quantiles of the log uncensored death times,
however users can supply their own knot locations in the \code{knots}
argument to \code{flexsurvspline}.

The function \code{dsurvspline} is provided by 

DPQR STATEMENTS


\subsection{General-dimension models}
\label{sec:gdim}

The spline model above is an example of the general parametric form
(\ref{eq:model}), but the number of parameters ($R+1$ in
(\ref{eq:model}), $m+2$ in (\ref{eq:spline})) is arbitrary.
\pkg{flexsurv} has the tools to deal with any model of this form.
\code{flexsurvspline} works internally by building a custom
distribution and then calling \code{flexsurvreg}.  Similar models may
in principle be built by users using the same method.  This relies on
a functional programming trick.

\paragraph{Creating distribution functions dynamically}

The R distribution functions supplied to custom models are expected to
have a fixed number of arguments, including one for each scalar
parameter.  However, the distribution functions for the spline model
(e.g. \code{dsurvspline}) have an argument \code{gamma} representing
the vector of parameters $\gamma$, whose length is determined by the
user through the choice of the number of knots.  Note also that the
\emph{scalar parameters} of conventional distribution functions can be
supplied as \emph{vector arguments}, and the vector parameters of
spline-like distribution functions can be supplied as \emph{matrix
  arguments}, representing alternative parameter values (REF BACK).

To convert a spline-like distribution function into the correct form,
\pkg{flexsurv} provides the utility \code{unroll.function}.  This
converts a function with one (or more) vector parameters (matrix
arguments) to a function with an arbitrary number of scalar parameters
(vector arguments).  For example, this shows two alternative ways of 
computing the 5-year survival probability for the baseline group 
under the model \code{sp1}. We tell \code{unroll.function} that the 
vector parameter \code{gamma} should be provided instead as three scalar parameters
named \code{gamma0},\code{gamma1},\code{gamma2}.  The resulting function \code{pfn}
is in the correct form for a custom \code{flexsurvreg} distribution.
\begin{Schunk}
\begin{Sinput}
> gamma <- sp1$res[c("gamma0","gamma1","gamma2"),"est"]
> 1 - psurvspline(5, gamma=gamma, knots=sp1$knots)
\end{Sinput}
\begin{Soutput}
[1] 0.6896969
\end{Soutput}
\begin{Sinput}
> pfn <- unroll.function(psurvspline, gamma=0:2)
> 1 - pfn(5, gamma0=gamma[1], gamma1=gamma[2], gamma2=gamma[3], knots=sp1$knots)
\end{Sinput}
\begin{Soutput}
[1] 0.6896969
\end{Soutput}
\end{Schunk}


\paragraph{Example: splines on alternative scales}

stgenreg has demo of spline modelling on the log hazard scale.  Can we do this using a generic distribution? 
(advantage: when there are multiple time dependent effects, the
interpretation of the time-dependent hazard ratios is simplified as
they do not depend on values of other covariates, which is the case
when modelling on the cumulative hazard scale (Royston and Lambert
2011).

\paragraph{Example: fractional polynomials}

%fractional polynomials? 
%specified by number M and set of powers p1,...pM
%chosen from c(-2,-1,-0.5,0,0.5,1,2,3), including repeats, where x^0 = log(x)
%with repeated powers, each repeat multiplied by a power of log(x)

relation to fractional polynomials \citep{royston1994regression}

Note that both splines and fractional polynomials have only been used
here to express how risk changes through time --- they can also be
used for expressing non-linear effects of any continuous predictor,
see e.g. \citet{sauerbrei2007selection}, and several other
publications by the same authors, for a discussion and comparison.
This may be achieved here by using a suitable basis function of the
covariate EXAMPLE in the model formula.

Polyhazard models \citep{polyhazard} are another potential application
of this technique.  These express an overall hazard as a sum of latent
cause-specific hazards, although they may suffer from lack of
identifiability.  cite nikos, me

\section{Multi-state models}

A \emph{multi-state model} represents how an individual moves between
multiple states through time.  Survival analysis is a special case of
multi-state modelling with two states ``alive'' and ``dead''. Suppose
an individual is in state $S(t)$ at time $t$.  The next state to which
the individual moves, and the time of the change, are governed by a
set of \emph{transition intensities} $q_{rs}(t)$ for states $r, s =
1,\dots,R$, which for a survival model are equivalent to the hazard
$h(t)$.  The intensity represents the instantaneous risk of moving
from state $r$ to state $s$.

Suppose our data consist of a series of event times $t_{1},\dots,
t_{n}$, the last of these may be an observed event or censoring.  Any
software to fit survival models can also fit multi-state models to
this kind of data, provided it can deal with left-truncation or
\emph{counting process} data.

For more discussion of the theory see \citet{putter:mstate}. ref also
Andersen for CP data

\paragraph{Counting process data}
For each permitted $r \rightarrow s$ transition in the multi-state
model (ILLUSTRATION) there is a corresponding \emph{time-to-event
  model}, with cause-specific hazard rates defined by $q_{rs}(t)$. To
enable estimation of these hazards, the data are expressed as a series
of times to events which are potentially censored: $dt_{j} = t_{j+1} -
t_{j}: j = 1,\ldots,n-1$. For a patient who moves into state $s$ at
time $t_{j}$, their next event at $t_{j+1}$ is defined by the model
structure (FIGURE) to be one of a set of
competing events $s^*_1,\ldots,s^*_{n_s}$.

For example, in state EXAMPLE, the next state must either be EXAMPLE
or EXAMPLE so $n_s=EG$.  The time of the event which actually occurs
at $t_{j+1}$ is \emph{observed}, and the times of the \emph{competing}
events from this set (which have not occurred by this time) are
\emph{censored}.  Each $dt_{j}$ contributes an \emph{observed} time to
one of the EG transition-specific models, and a \emph{censored} time
to each of the models for the competing events.

FLEXSURVREG EXAMPLE

The \pkg{mstate} R package \citep{mstate:cmpb,mstate:jss} has a
utility \code{msprep} to produce data of this form from
``wide-format'' datasets where rows represent individuals, and times
of different events appear in different columns, and \pkg{mstate} has
a utility \code{msm2Surv} for 

illustrates Cox models
flexible parametric multi-state models 

\paragraph{Prediction from multi-state models}

Define cumulative incidence functions

The \code{mstate} package is designed to work with 
piecewise-constant cumulative incidence functions
baseline hazards are estimated non-parametrically 
\citep{mstate:cmpb,mstate:jss} 

function \code{msfit} that produces the cumulative incidences for each transition and a given covariate category, and their covariances, given a Cox model fitted using \code{coxph} from the \pkg{survival} package. 

Aalen-Johansen estimator,  simulation

contrast Markov and semi-Markov models

\paragraph{Multi-state models for panel data}

Note the contrast with multi-state models for \emph{panel data}, that is,
observations of the state of the process at a series of times
\citep{kalbfleisch:lawless}.  In panel data, we do not necessarily
know know the time of each transition, or even whether a transitions
of a certain type have occurred at all between a pair of observations.
Such models can be fitted with the \pkg{msm} package for R, but are
restricted to (piecewise) exponentially-distributed event times.



\code{survSplit} function in \pkg{survival} 


% cif in comp risks planned for stgenreg 
\section{Potential extensions}

relative survival
frailty 
many extensions may come from user-contributed models


\appendix
\section{Acknowledgements}
Thanks to Milan Bouchet-Valat.

\bibliography{flexsurv}

\end{document}
